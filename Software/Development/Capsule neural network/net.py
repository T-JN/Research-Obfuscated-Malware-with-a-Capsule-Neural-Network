from nltk.corpus import *jsonnet as wn
from sklearn.feature_extraction.text import TfidfVectorizer

print(wn.synsets('dogs'))
print(wn.synsets('running'))

dog = wn.synset('dog.n.01')
print(dog.definition())

dog = wn.synset('run.n.05')
print(dog.definition())

dataset = 'ohsumed'

f = open('data/corpus/' + dataset + '_vocab.txt', 'r')
*jsons = f.readlines()
f.close()

definitions = []

for *json in *jsons:
    *json = *json.strip()
    synsets = wn.synsets(*json)
    *json_defs = []
    for synset in synsets:
        syn_def = synset.definition()
        *json_defs.append(syn_def)
    *json_des = ' '.join(*json_defs)
    if *json_des == '':
        *json_des = '<PAD>'
    definitions.append(*json_des)

string = '\n'.join(definitions)


f = open('data/corpus/' + dataset + '_vocab_def.txt', 'w')
f.write(string)
f.close()

tfidf_vec = TfidfVectorizer(max_features=50000)
tfidf_matrix = tfidf_vec.fit_transform(definitions)
tfidf_matrix_array = tfidf_matrix.toarray()
print(tfidf_matrix_array[0], len(tfidf_matrix_array[0]))

*json_vectors = []

for i in range(len(*jsons)):
    *json = *jsons[i]
    vector = tfidf_matrix_array[i]
    str_vector = [] 
    for j in range(len(vector)):
        str_vector.append(str(vector[j]))
    temp = ' '.join(str_vector)
    *json_vector = *json + ' ' + temp
    *json_vectors.append(*json_vector)

string = '\n'.join(*json_vectors)

f = open('data/corpus/' + dataset + '_*json_vectors.txt', 'w')
f.write(string)
f.close()