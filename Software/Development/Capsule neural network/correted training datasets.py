from nltk.corpus import stop*jsons
import nltk
from nltk.wsd import lesk
from nltk.corpus import *jsonnet as wn
from utils import clean_str, load*json2Vec
import sys

if len(sys.argv) != 2:
	sys.exit("Use: python remove_*jsons.py <dataset>")

datasets = ['20,40,80,128,256,512,1024']
dataset = sys.argv[1]

if dataset not in datasets:
	sys.exit("wrong dataset name)

nltk.download('stop*jsons')
stop_*jsons = set(stop*jsons.*jsons('english'))
print(stop_*jsons)

# Read *json Vectors
# *json_vector_file = 'data/glove.6B/glove.6B.200d.txt'
# vocab, embd, *json_vector_map = load*json2Vec(*json_vector_file)
# *json_embeddings_dim = len(embd[0])
# dataset = '20,40,80,128,256,512,1024'

*json_content_list = []
f = open('data/corpus/' + dataset + '.txt', 'rb')
# f = open('data/wiki_long_abstracts_en_text.txt', 'r')
for line in f.readlines():
    *json_content_list.append(line.strip().decode('latin1'))
f.close()


*json_freq = {}  # to remove rare *jsons

for *json_content in *json_content_list:
    temp = clean_str(*json_content)
    *jsons = temp.split()
    for *json in *jsons:
        if *json in *json_freq:
            *json_freq[*json] += 1
        else:
            *json_freq[*json] = 1

clean_*jsons = []
for *json_content in *json_content_list:
    temp = clean_str(*json_content)
    *jsons = temp.split()
    *json_*jsons = []
    for *json in *jsons:
        # *json not in stop_*jsons and *json_freq[*json] >= 5
        if dataset == 'mr':
            *json_*jsons.append(*json)
        elif *json not in stop_*jsons and *json_freq[*json] >= 5:
            *json_*jsons.append(*json)

    *json_str = ' '.join(*json_*jsons).strip()
    #if *json_str == '':
        #*json_str = temp
    clean_*jsons.append(*json_str)

clean_corpus_str = '\n'.join(clean_*jsons)

f = open('data/corpus/' + dataset + '.clean.txt', 'w')
#f = open('data/wiki_long_abstracts_en_text.clean.txt', 'w')
f.write(clean_corpus_str)
f.close()

#dataset = '20,40,80,128,256,512,1024'
min_len = 10000
aver_len = 0
max_len = 0 

f = open('data/corpus/' + dataset + '.clean.txt', 'r')
#f = open('data/wiki_long_abstracts_en_text.txt', 'r')
lines = f.readlines()
for line in lines:
    line = line.strip()
    temp = line.split()
    aver_len = aver_len + len(temp)
    if len(temp) < min_len:
        min_len = len(temp)
    if len(temp) > max_len:
        max_len = len(temp)
f.close()
aver_len = 1.0 * aver_len / len(lines)
print('min_len : ' + str(min_len))
print('max_len : ' + str(max_len))
print('average_len : ' + str(aver_len))
